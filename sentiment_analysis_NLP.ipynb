{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "j7Eh5Pdhaxmv"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnnqFZN7bpU9",
        "outputId": "58c1b3c2-5a4e-49f4-b548-ec5dbac6ceb9"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words=set(stopwords.words('english'))\n",
        "stop_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sknz6jB1bx1P",
        "outputId": "804894ca-c71d-4d4b-c097-50062e962927"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " \"he'd\",\n",
              " \"he'll\",\n",
              " \"he's\",\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " \"i'd\",\n",
              " \"i'll\",\n",
              " \"i'm\",\n",
              " \"i've\",\n",
              " 'if',\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it'd\",\n",
              " \"it'll\",\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she'd\",\n",
              " \"she'll\",\n",
              " \"she's\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " \"they'd\",\n",
              " \"they'll\",\n",
              " \"they're\",\n",
              " \"they've\",\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " \"we'd\",\n",
              " \"we'll\",\n",
              " \"we're\",\n",
              " \"we've\",\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves'}"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "t=['loves','loving', 'loved', 'lovely','beautiful']\n",
        "lemmatized_list=[lemmatizer.lemmatize(word,'v') for word in t]\n",
        "lemmatized_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mw45j7KZd0RN",
        "outputId": "213d3ac0-2339-4ebf-891c-8a95fe6af5cb"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['love', 'love', 'love', 'lovely', 'beautiful']"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer=PorterStemmer()\n",
        "t=['loves','loving', 'loved', 'lovely','beautiful']\n",
        "stemmed_list=[stemmer.stem(word) for word in t]\n",
        "stemmed_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_H4j2t7coXL",
        "outputId": "d788ed62-74db-47bc-ddb4-d07279d14204"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['love', 'love', 'love', 'love', 'beauti']"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=[\"I love this product\",\n",
        "    \"This is the worst experience\",\n",
        "    \"Amazing quality and great support\",\n",
        "    \"I hate this so much\",\n",
        "    \"Not bad, could be better\",\n",
        "    \"Absolutely fantastic\"]\n",
        "labels=[1,0,1,0,0,1]"
      ],
      "metadata": {
        "id": "W6UOJFQiaSi8"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "  if isinstance(text, list):\n",
        "    return [preprocess(t) for t in text]\n",
        "  text=text.lower()\n",
        "  text=re.sub(r'[^a-z\\s]','', text)\n",
        "  words=text.split()\n",
        "  words=[stemmer.stem(word) for word in words if word not in stop_words]\n",
        "  return \" \".join(words)\n",
        "preprocess(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPJ-8akfiU9i",
        "outputId": "33d73e8b-8003-47b7-c042-5a9a84eec437"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['love product',\n",
              " 'worst experi',\n",
              " 'amaz qualiti great support',\n",
              " 'hate much',\n",
              " 'bad could better',\n",
              " 'absolut fantast']"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_cleaned=preprocess(text)\n",
        "text_cleaned"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxpDKkLlq-vG",
        "outputId": "db0db49b-5671-452f-8d6f-d67c4a7b6548"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['love product',\n",
              " 'worst experi',\n",
              " 'amaz qualiti great support',\n",
              " 'hate much',\n",
              " 'bad could better',\n",
              " 'absolut fantast']"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"I love NLP\",\n",
        "    \"I love Python\",\n",
        "    \"Python loves data\",\n",
        "    \"Today is a very beautiful day\",\n",
        "    \"I love beautiful peoples \",\n",
        "    \"we all are a part of family\",\n",
        "    \" a b c d e f g h i j k l m n  o p q r s t u v w x y z\"\n",
        "]"
      ],
      "metadata": {
        "id": "eizKLhGYs0cK"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer=CountVectorizer()\n",
        "X = vectorizer.fit_transform(text)\n",
        "vectorizer.vocabulary_\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtJYOAOtr25z",
        "outputId": "ef355c65-b2fe-4fce-8d4f-977521d25e97"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'love': 12,\n",
              " 'this': 20,\n",
              " 'product': 15,\n",
              " 'is': 11,\n",
              " 'the': 19,\n",
              " 'worst': 21,\n",
              " 'experience': 7,\n",
              " 'amazing': 1,\n",
              " 'quality': 16,\n",
              " 'and': 2,\n",
              " 'great': 9,\n",
              " 'support': 18,\n",
              " 'hate': 10,\n",
              " 'so': 17,\n",
              " 'much': 13,\n",
              " 'not': 14,\n",
              " 'bad': 3,\n",
              " 'could': 6,\n",
              " 'be': 4,\n",
              " 'better': 5,\n",
              " 'absolutely': 0,\n",
              " 'fantastic': 8}"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"imdb\")\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ibRt8KriJRT",
        "outputId": "dbd7fa5f-0eb1-46b3-85b9-01822eafe5e6"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    unsupervised: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 50000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=dataset[\"train\"]['text']\n",
        "y_train = dataset[\"train\"]['label']\n",
        "\n",
        "X_test = dataset[\"test\"][\"text\"]\n",
        "y_test = dataset['test']['label']\n",
        "\n",
        "print(len(X_train), len(y_train))\n",
        "print(len(X_test), len(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tkxpbze6iSd2",
        "outputId": "debe648b-1098-4b07-eb4b-4ca20fe3e34e"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25000 25000\n",
            "25000 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def clean_review(text):\n",
        "  text = re.sub(r'<.*?>', ' ', text)\n",
        "  text = re.sub(r'\\s+', ' ', text)\n",
        "  return text.strip()"
      ],
      "metadata": {
        "id": "HulD6iIkjo_2"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_clean=[clean_review(r) for r in X_train]\n",
        "X_test_clean = [clean_review(r) for r in X_test]"
      ],
      "metadata": {
        "id": "DMU1qjoukWpV"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(\n",
        "    lowercase=True,\n",
        "    ngram_range=(1,2),\n",
        "    min_df=5,\n",
        "    max_df=0.9\n",
        ")\n",
        "\n",
        "X_train_vec=vectorizer.fit_transform(X_train_clean)\n",
        "X_test_vec = vectorizer.transform(X_test_clean)"
      ],
      "metadata": {
        "id": "DrBxsrWGlDgS"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_vec, y_train);"
      ],
      "metadata": {
        "id": "fdLG8nRiXizz"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(X_test_vec)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpzVBfWMaIw6",
        "outputId": "18977295-724a-43ec-c57f-705a9299034c"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.89372\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {0: \"Negative\", 1: \"Positive\"}\n",
        "\n",
        "def predict_label(sentence):\n",
        "    clean = clean_review(sentence)\n",
        "    vec = vectorizer.transform([clean])\n",
        "    label = model.predict(vec)[0]\n",
        "    return label_map[label]\n"
      ],
      "metadata": {
        "id": "IMuoukAPaVlu"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict_label(\"The movie was boring but the ending was great\"))\n",
        "print(predict_label(\"It was a really an lucky and a very great moment in my life\"))\n",
        "print(predict_label(\"This was a complete waste of time\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bCVCzasYiF3",
        "outputId": "25c58d59-be79-4a79-c542-42d431b69b4f"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative\n",
            "Positive\n",
            "Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = [\n",
        "    \"I absolutely loved this movie. The performances were brilliant and the story was engaging from start to finish.\",\n",
        "    \"This film was a complete waste of time. The plot was boring and the acting was terrible.\",\n",
        "    \"The movie was simple and enjoyable. Nothing extraordinary, but it was pleasant to watch.\",\n",
        "    \"The idea was interesting, but the execution was poor and the pacing was very slow.\",\n",
        "    \"The movie started well, but the second half was disappointing and ruined the experience.\",\n",
        "    \"What an amazing film! I was completely absorbed and emotionally invested throughout.\",\n",
        "    \"I hated every minute of this movie. It was confusing, dull, and badly written.\",\n",
        "    \"The movie was not good and definitely not worth the hype.\",\n",
        "    \"Despite a slow start, the film turns into a powerful and memorable experience.\",\n",
        "    \"It was okay. Some parts were forgettable, others were interesting.\"\n",
        "]\n",
        "\n",
        "for r in reviews:\n",
        "    print(predict_label(r), \"→\", r)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3g3LZQbwbGtm",
        "outputId": "22c3454f-bba1-4a56-c774-e9ac430ad960"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive → I absolutely loved this movie. The performances were brilliant and the story was engaging from start to finish.\n",
            "Negative → This film was a complete waste of time. The plot was boring and the acting was terrible.\n",
            "Positive → The movie was simple and enjoyable. Nothing extraordinary, but it was pleasant to watch.\n",
            "Negative → The idea was interesting, but the execution was poor and the pacing was very slow.\n",
            "Negative → The movie started well, but the second half was disappointing and ruined the experience.\n",
            "Positive → What an amazing film! I was completely absorbed and emotionally invested throughout.\n",
            "Negative → I hated every minute of this movie. It was confusing, dull, and badly written.\n",
            "Negative → The movie was not good and definitely not worth the hype.\n",
            "Positive → Despite a slow start, the film turns into a powerful and memorable experience.\n",
            "Negative → It was okay. Some parts were forgettable, others were interesting.\n"
          ]
        }
      ]
    }
  ]
}